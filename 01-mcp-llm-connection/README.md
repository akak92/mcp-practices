# 01-mcp-llm-connection

## DescripciÃ³n
Esta carpeta contiene un ejemplo avanzado de **Model Context Protocol (MCP)** que demuestra la integraciÃ³n entre un servidor MCP y un **LLM local** (qwen3:1.7b via Ollama) usando un **BFF proxy**. Este proyecto muestra cÃ³mo los LLMs pueden descubrir y utilizar herramientas MCP de forma inteligente y autÃ³noma.

## Â¿QuÃ© es este Proyecto?
Este ejemplo implementa un flujo completo donde:
1. Un **servidor MCP** expone herramientas disponibles
2. Un **cliente inteligente** conecta el servidor MCP con un LLM local
3. El **LLM** puede descubrir, analizar y ejecutar herramientas segÃºn las consultas del usuario
4. Todo funciona **localmente** sin enviar datos a servicios externos

## Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario       â”‚â”€â”€â”€â–¶â”‚  Cliente MCP    â”‚â”€â”€â”€â–¶â”‚  BFF Proxy      â”‚â”€â”€â”€â–¶â”‚ Ollama (qwen3)  â”‚
â”‚   "Suma 15+25"  â”‚    â”‚  (client.py)    â”‚    â”‚ localhost:9900  â”‚    â”‚ LLM Local       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                        â–²
                               â–¼                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
                       â”‚  Servidor MCP   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  (server.py)    â”‚ Ejecuta herramientas
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Estructura del Proyecto

```
01-mcp-llm-connection/
â”œâ”€â”€ server.py          # Servidor MCP con herramientas
â”œâ”€â”€ client.py          # Cliente inteligente con integraciÃ³n LLM
â”œâ”€â”€ utils/             # MÃ³dulos de utilidades
â”‚   â”œâ”€â”€ __init__.py    # InicializaciÃ³n del mÃ³dulo
â”‚   â””â”€â”€ tools.py       # Herramientas matemÃ¡ticas
â”œâ”€â”€ requirements.txt   # Dependencias Python
â”œâ”€â”€ Dockerfile         # Imagen Docker optimizada
â”œâ”€â”€ .dockerignore      # Archivos a ignorar en Docker
â”œâ”€â”€ run_docker.ps1     # Script Docker para Windows
â””â”€â”€ run_docker.sh      # Script Docker para Unix
```

## Componentes Principales

### 1. Servidor MCP (`server.py`)
Servidor simplificado que expone:
- **Herramienta**: `add_tool(a: int, b: int)` - Suma dos nÃºmeros
- **Protocolo**: FastMCP con comunicaciÃ³n stdio
- **PropÃ³sito**: Demostrar cÃ³mo un LLM puede usar herramientas externas

### 2. Cliente Inteligente (`client.py`)
El componente mÃ¡s complejo que implementa:

#### **Funcionalidades Principales:**
- **ConexiÃ³n MCP**: Se conecta al servidor via stdio
- **IntegraciÃ³n LLM**: Comunica con qwen3 via BFF proxy
- **ConversiÃ³n de Esquemas**: Adapta herramientas MCP al formato OpenAI Functions
- **DetecciÃ³n Inteligente**: Analiza respuestas del LLM para identificar uso de herramientas
- **EjecuciÃ³n AutomÃ¡tica**: Ejecuta herramientas detectadas y devuelve resultados
- **Logging Profesional**: Sistema de logs estructurado con timestamps

#### **Flujo de Procesamiento:**
```python
async def run():
    # 1. Conectar al servidor MCP
    # 2. Descubrir herramientas disponibles
    # 3. Convertir esquemas para el LLM
    # 4. Enviar consulta al LLM con contexto de herramientas
    # 5. Analizar respuesta del LLM
    # 6. Detectar y ejecutar herramientas mencionadas
    # 7. Mostrar resultados finales
```

### 3. BFF Proxy Integration
El cliente se comunica con un **Backend for Frontend (BFF)** que actÃºa como proxy hacia Ollama:

```python
async def call_llm(prompt, functions):
    url = "http://localhost:9900/chat"
    payload = {
        "model": "qwen3:1.7b",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }
    # El BFF maneja la comunicaciÃ³n con Ollama
```

**Formato de Respuesta del BFF:**
```json
{
    "content": "UsÃ© la herramienta add_tool con parÃ¡metros a=15 y b=25. El resultado es 40.",
    "model": "qwen3:1.7b",
    "usage": {"model": "qwen3:1.7b"}
}
```

### 4. Sistema de Logging
ImplementaciÃ³n profesional de logging:

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

**Tipos de Logs:**
- `âœ… Connected to MCP server` - ConexiÃ³n exitosa
- `ğŸ”§ LISTING TOOLS` - Descubrimiento de herramientas
- `ğŸ¤– CALLING BFF PROXY` - ComunicaciÃ³n con LLM
- `ğŸ’¬ LLM Response` - Respuestas del modelo
- `ğŸ¯ Detected tool usage` - DetecciÃ³n de uso de herramientas

## Requisitos del Sistema

### Software Necesario
- **Python 3.8+**
- **Ollama** con modelo qwen3:1.7b
- **BFF Proxy** ejecutÃ¡ndose en `localhost:9900`
- **Docker** (opcional, para containerizaciÃ³n)

### Dependencias Python
```
fastmcp
httpx
asyncio
logging
```

## InstalaciÃ³n y ConfiguraciÃ³n

### 1. Configurar Ollama
```bash
# Instalar Ollama
# Descargar modelo qwen3:1.7b
ollama pull qwen3:1.7b

# Verificar que funciona
ollama run qwen3:1.7b
```

### 2. Configurar BFF Proxy
El BFF debe estar ejecutÃ¡ndose en `localhost:9900` y aceptar requests POST a `/chat`.

### 3. InstalaciÃ³n Local
```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el proyecto
python client.py
```

### 4. Usando Docker
```bash
# Construir imagen
docker build -t mcp-llm-client .

# Ejecutar con acceso a localhost
docker run --rm --network="host" mcp-llm-client
```

## Flujo de Funcionamiento Detallado

### Fase 1: InicializaciÃ³n
1. **Cliente** se conecta al **Servidor MCP** via stdio
2. **Cliente** descubre herramientas disponibles (`add_tool`)
3. **Cliente** convierte esquemas MCP al formato OpenAI Functions

### Fase 2: Procesamiento de Consulta
1. **Usuario** hace una pregunta: *"Can you help me add 15 and 25?"*
2. **Cliente** envÃ­a consulta al **LLM** via **BFF Proxy**
3. **LLM** analiza la consulta y el contexto de herramientas disponibles
4. **LLM** responde indicando quÃ© herramienta usar y con quÃ© parÃ¡metros

### Fase 3: EjecuciÃ³n Inteligente
1. **Cliente** analiza la respuesta del **LLM** con regex/parsing
2. **Cliente** detecta: *"add_tool con parÃ¡metros a=15 y b=25"*
3. **Cliente** ejecuta la herramienta en el **Servidor MCP**
4. **Servidor** procesa `add_tool(15, 25)` y devuelve `40`

### Fase 4: Resultado Final
1. **Cliente** recibe el resultado de la herramienta
2. **Cliente** muestra tanto la respuesta del **LLM** como el resultado real
3. **Logging** registra todo el flujo para debugging

## Ejemplo de EjecuciÃ³n

### Input del Usuario:
```
Can you help me add 15 and 25?
```

### Logs del Sistema:
```
2025-11-05 01:31:28,770 - __main__ - INFO - âœ… Connected to MCP server
2025-11-05 01:31:28,774 - __main__ - INFO - ğŸ“ LISTING RESOURCES
2025-11-05 01:31:28,778 - __main__ - INFO - ğŸ”§ LISTING TOOLS
2025-11-05 01:31:28,778 - __main__ - INFO - Tool: add_tool - Add two numbers
2025-11-05 01:31:28,778 - __main__ - INFO - ğŸ¯ User Query: Can you help me add 15 and 25?
2025-11-05 01:31:28,778 - __main__ - INFO - ğŸ¤– CALLING BFF PROXY at http://localhost:9900/chat
2025-11-05 01:31:36,791 - __main__ - INFO - ğŸ’¬ LLM Response: UsÃ© la herramienta add_tool con parÃ¡metros a=15 y b=25. El resultado es 40.
2025-11-05 01:31:36,791 - __main__ - INFO - ğŸ¯ Detected add_tool usage with numbers: 15 + 25
2025-11-05 01:31:36,791 - __main__ - INFO - ğŸ”§ Executing 1 tool call(s):
2025-11-05 01:31:36,801 - __main__ - INFO - Result: 40
```

### Output Final:
```
LLM Response: UsÃ© la herramienta add_tool con parÃ¡metros a=15 y b=25. El resultado es 40.
Tool Result: 40
```

## CaracterÃ­sticas TÃ©cnicas Avanzadas

### 1. ConversiÃ³n de Esquemas
El cliente convierte automÃ¡ticamente esquemas MCP al formato OpenAI Functions:

```python
def convert_to_llm_tool(tool):
    return {
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description,
            "parameters": {
                "type": "object",
                "properties": tool.inputSchema.get("properties", {}),
                "required": list(tool.inputSchema.get("properties", {}).keys())
            }
        }
    }
```

### 2. DetecciÃ³n Inteligente de Herramientas
Usa regex para detectar uso de herramientas en respuestas del LLM:

```python
# Detecta patrones como "add_tool con parÃ¡metros a=15 y b=25"
tool_pattern = r'add_tool.*?a=(\d+).*?b=(\d+)'
match = re.search(tool_pattern, llm_response)
```

### 3. Manejo de Errores Robusto
- Timeout handling para requests HTTP
- Fallback si el LLM no usa herramientas
- Logging detallado para debugging
- ValidaciÃ³n de parÃ¡metros de herramientas

### 4. DockerizaciÃ³n con Host Networking
```dockerfile
# Permite acceso a localhost desde el contenedor
docker run --network="host" mcp-llm-client
```

## Casos de Uso

### 1. **Asistente MatemÃ¡tico Local**
- Usuario pregunta operaciones matemÃ¡ticas
- LLM identifica la operaciÃ³n necesaria
- Sistema ejecuta cÃ¡lculos precisos

### 2. **Prototipo de IA Tool-Using**
- Demostrar capacidades de LLMs para usar herramientas
- Testing de integraciÃ³n MCP-LLM
- Desarrollo de sistemas de IA mÃ¡s complejos

### 3. **EducaciÃ³n en MCP**
- Entender cÃ³mo LLMs pueden usar herramientas externas
- Aprender patrones de integraciÃ³n MCP
- Base para proyectos mÃ¡s avanzados

## Ventajas del Enfoque

### âœ… **Privacidad Total**
- Todo funciona localmente
- No se envÃ­an datos a servicios externos
- Control completo sobre el procesamiento

### âœ… **Extensibilidad**
- FÃ¡cil agregar nuevas herramientas al servidor MCP
- LLM automÃ¡ticamente las descubre y puede usarlas
- Arquitectura modular y escalable

### âœ… **Observabilidad**
- Logging detallado de todo el flujo
- Debugging fÃ¡cil con timestamps
- Visibilidad completa del proceso

### âœ… **ContainerizaciÃ³n**
- Deploy fÃ¡cil con Docker
- Ambiente reproducible
- Aislamiento de dependencias

## PrÃ³ximos Pasos y Mejoras

### ğŸš€ **ExpansiÃ³n de Herramientas**
- Agregar mÃ¡s herramientas matemÃ¡ticas
- Implementar herramientas de texto
- Conectar con APIs externas

### ğŸš€ **Mejora de IA**
- Usar modelos mÃ¡s grandes
- Implementar chain-of-thought
- Agregar memoria conversacional

### ğŸš€ **OptimizaciÃ³n**
- Caching de respuestas
- ParalelizaciÃ³n de herramientas
- OptimizaciÃ³n de prompts

### ğŸš€ **Monitoreo**
- MÃ©tricas de performance
- Alertas de errores
- Dashboard de uso

## ConclusiÃ³n

Este proyecto demuestra una implementaciÃ³n exitosa de **MCP + LLM local**, creando un sistema que permite a un modelo de lenguaje descubrir y utilizar herramientas externas de forma inteligente, manteniendo todo el procesamiento local y con total control sobre los datos.